{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Movie                                               Tags\n",
      "0     1292052  [弗兰克达拉邦特, 【美】, 释放, 监狱内外, 94, 自由与希望, 1990~1999,...\n",
      "1     1295644  [吕克·贝松　让·雷诺　娜塔利波特曼, 法国, 娜塔莉波曼, 电影院, GaryOldman...\n",
      "2     1292720  [阿甘正传, 记录米国, TOM, 【美】, 爱情, Zemeckis, D罗伯特·泽米吉斯...\n",
      "3     3541415  [幻想科幻魔幻系, 科幻, UK, 2010-2019, 沟通障碍, 四星下, 李奥纳多, ...\n",
      "4     3742360  [国货电影们, 红旗飘飘, 姜文, 四川话版, 沟通障碍, online.streaming...\n",
      "...       ...                                                ...\n",
      "1195  2357711  [日本动漫, 00, 东洋动画, 动漫, 亚洲作品, 动画, 卡通片, carton, 日本...\n",
      "1196  1295873  [B&W, 烧钱CC版, 中文配音, 10年5月看过, EnzoStaiola, CSC, ...\n",
      "1197  1997681  [大陆电影, 2005, 国货电影们, 中国内地, 找乐, 国产, 中国大陆, 电影频道, ...\n",
      "1198  1827955  [動畫, 00, 动漫, 3星半, 喜欢的。, 亚洲作品, 动画, 卡通片, 动作, car...\n",
      "1199  1307181  [2003年, 西方Movie, 政治, 威瑟斯彭, 爱情, .电影, 喜劇, ChickF...\n",
      "\n",
      "[1200 rows x 2 columns]\n",
      "           User    Movie  Rate\n",
      "1       1386692  1986338     3\n",
      "2       1386692  4268598     5\n",
      "3       1386692  1851857     4\n",
      "4       1386692  4023638     4\n",
      "5       1386692  1305903     3\n",
      "...         ...      ...   ...\n",
      "714934  1379646  1309004     5\n",
      "714967  1379646  1783772     5\n",
      "714984  1379646  1291859     5\n",
      "714993  1379646  1484091     5\n",
      "715018  1379646  1291836     4\n",
      "\n",
      "[523514 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tag 数据集\n",
    "# 读tag_data取保存的 CSV 文件\n",
    "tag_data = pd.read_csv('../data/selected_movie_top_1200_data_tag.csv')\n",
    "# 引入用户评价数据\n",
    "rating_data = pd.read_csv('../data/movie_score.csv')\n",
    "# 删除 NaN 的行\n",
    "rating_data.dropna(inplace=True)\n",
    "# 将用户打的 Tag 加入到 tag_data['Tags'] 中\n",
    "tag_data['Tags'] = tag_data['Movie'].map(rating_data.groupby('Movie')['Tags'].apply(list).to_dict())\n",
    "# 对于每一行 Tags，将其转换为一整个字符串\n",
    "tag_data['Tags'] = tag_data['Tags'].apply(lambda x: ','.join(x))\n",
    "# 拆分为列表，去重，去除空字符串，'|', '...' 等无意义的 Tag\n",
    "tag_data['Tags'] = tag_data['Tags'].apply(lambda x: list(set(x.split(','))))\n",
    "tag_data['Tags'] = tag_data['Tags'].apply(lambda x: list(filter(lambda x: x not in ['', '|', '...'], x)))\n",
    "# 保存为 CSV 文件\n",
    "tag_data.to_csv('../res/selected_tags.csv', index=False)\n",
    "print(tag_data)\n",
    "\n",
    "\n",
    "# User 数据集\n",
    "# 读user_data取保存的 CSV 文件\n",
    "user_data = pd.read_csv('../data/movie_score.csv')\n",
    "# 去除评分为 0 的行？\n",
    "user_data = user_data[user_data['Rate'] > 0]\n",
    "# 去除评价数据过少的用户\n",
    "user_data = user_data.groupby('User').filter(lambda x: len(x) > 10)\n",
    "# 去除不必要的列\n",
    "user_data = user_data[['User', 'Movie', 'Rate']]\n",
    "# 保存为 CSV 文件\n",
    "user_data.to_csv('../res/selected_users.csv', index=False)\n",
    "print(user_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [00:31, 37.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Bert 预训练模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese').cuda()\n",
    "\n",
    "# 保存标签嵌入向量\n",
    "tag_embedding_dict = {}\n",
    "with torch.no_grad():\n",
    "    for index, rows in tqdm(tag_data.iterrows()):\n",
    "        # 将标签列表转换为字符串\n",
    "        tags_str = \"\".join(rows.Tags)\n",
    "        # 使用BERT中文模型对标签进行编码\n",
    "        inputs = tokenizer(tags_str, truncation=True, return_tensors='pt')\n",
    "        outputs = model(inputs.input_ids.cuda(), inputs.token_type_ids.cuda(), inputs.attention_mask.cuda())\n",
    "        # 使用最后一层的平均隐藏状态作为标签的向量表示\n",
    "        tag_embedding = outputs.last_hidden_state.mean(dim=1).cpu()\n",
    "        tag_embedding_dict[rows.Movie] = tag_embedding\n",
    "\n",
    "# 将映射表存储为二进制文件\n",
    "with open('../res/tag_embedding_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(tag_embedding_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集类\n",
    "class MovieRatingDataset(Dataset):\n",
    "    def __init__(self, data, user_to_idx, movie_to_idx, tag_embedding_dict):\n",
    "        self.data = data\n",
    "        self.user_to_idx = user_to_idx\n",
    "        self.movie_to_idx = movie_to_idx\n",
    "        self.tag_embedding_dict = tag_embedding_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        user = self.user_to_idx[row['User']]\n",
    "        movie = self.movie_to_idx[row['Movie']]\n",
    "        rating = row['Rate'].astype('float32')\n",
    "        text_embedding = self.tag_embedding_dict.get(row['Movie'])\n",
    "        return user, movie, rating, text_embedding\n",
    "    \n",
    "# 创建索引映射\n",
    "def create_id_mapping(id_list):\n",
    "    # 从ID列表中删除重复项并创建一个排序的列表\n",
    "    unique_ids = sorted(set(id_list))\n",
    "    \n",
    "    # 创建将原始ID映射到连续索引的字典\n",
    "    id_to_idx = {id: idx for idx, id in enumerate(unique_ids)}\n",
    "    \n",
    "    # 创建将连续索引映射回原始ID的字典\n",
    "    idx_to_id = {idx: id for id, idx in id_to_idx.items()}\n",
    "    \n",
    "    return id_to_idx, idx_to_id\n",
    "\n",
    "user_ids = user_data['User'].unique()\n",
    "movie_ids = user_data['Movie'].unique()\n",
    "user_to_idx, idx_to_user = create_id_mapping(user_ids)\n",
    "movie_to_idx, idx_to_movie = create_id_mapping(movie_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型，引入 Item User 偏置提高效果\n",
    "class MF(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim, init_std = 0.1):\n",
    "        super(MF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.movie_bias = nn.Embedding(num_movies, 1)\n",
    "        nn.init.normal_(self.user_embedding.weight, std = init_std)\n",
    "        nn.init.normal_(self.movie_embedding.weight, std = init_std)\n",
    "        nn.init.normal_(self.user_bias.weight, std = init_std)\n",
    "        nn.init.normal_(self.movie_bias.weight, std = init_std)\n",
    "        \n",
    "    def forward(self, user, movie):\n",
    "        user_embedding = self.user_embedding(user)\n",
    "        movie_embedding = self.movie_embedding(movie)\n",
    "        user_bias = self.user_bias(user)\n",
    "        movie_bias = self.movie_bias(movie)\n",
    "        dot = (user_embedding * movie_embedding).sum(1)\n",
    "        return dot + user_bias.squeeze() + movie_bias.squeeze()\n",
    "\n",
    "# 定义模型\n",
    "model = MF(len(user_to_idx), len(movie_to_idx), 768).cuda()\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:06,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 6.054512022979676, Test loss:, 1.4061590433120728, Average NDCG: 0.8829743819420974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.8401029687079172, Test loss:, 1.398927858897618, Average NDCG: 0.8918389251537128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:08,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.4745940693787166, Test loss:, 1.4090804467125544, Average NDCG: 0.8968032849112698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:08,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 0.3461354020096007, Test loss:, 1.3801041273843675, Average NDCG: 0.8969722755005403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss: 0.29199493688250344, Test loss:, 1.4091123531735132, Average NDCG: 0.8970383364984921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train loss: 0.2691051936338818, Test loss:, 1.4086632917797755, Average NDCG: 0.8977216694274244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train loss: 0.2586870997671097, Test loss:, 1.3667410214742024, Average NDCG: 0.8986278580910495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train loss: 0.2534109590545533, Test loss:, 1.3544818389983404, Average NDCG: 0.8996325808535107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train loss: 0.25037265604450587, Test loss:, 1.344119049253918, Average NDCG: 0.9006444676357661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train loss: 0.24828930743156918, Test loss:, 1.3704514219647361, Average NDCG: 0.901755235871216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train loss: 0.2462947080059657, Test loss:, 1.3622532117934454, Average NDCG: 0.9023203863048813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train loss: 0.24528758795488448, Test loss:, 1.3491712959985884, Average NDCG: 0.9036339944544485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train loss: 0.24496426587066952, Test loss:, 1.3249568163402496, Average NDCG: 0.9040977712929896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train loss: 0.2455528693066703, Test loss:, 1.3259062899483576, Average NDCG: 0.9048873460975086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train loss: 0.2467158500637327, Test loss:, 1.3467443311025227, Average NDCG: 0.9056492426958429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train loss: 0.24945490819121163, Test loss:, 1.2952807290213448, Average NDCG: 0.905833571444622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train loss: 0.25416723130241275, Test loss:, 1.3335483244487218, Average NDCG: 0.9058622164132282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:06,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train loss: 0.26187914941045976, Test loss:, 1.3459090145807417, Average NDCG: 0.9068220355050349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train loss: 0.27371161844995284, Test loss:, 1.318461393553113, Average NDCG: 0.9060660361338378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train loss: 0.2916186697899349, Test loss:, 1.3157087148181976, Average NDCG: 0.9052107788269304\n"
     ]
    }
   ],
   "source": [
    "# 按用户分组计算NDCG\n",
    "def compute_ndcg(group):\n",
    "    true_ratings = group['true'].tolist()\n",
    "    pred_ratings = group['pred'].tolist()\n",
    "    return ndcg_score([true_ratings], [pred_ratings], k = 50)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_data, test_data = train_test_split(user_data, test_size=0.5, random_state=42)\n",
    "# 创建训练集和测试集的数据集对象\n",
    "train_dataset = MovieRatingDataset(train_data, user_to_idx, movie_to_idx, tag_embedding_dict)\n",
    "test_dataset = MovieRatingDataset(test_data, user_to_idx, movie_to_idx, tag_embedding_dict)\n",
    "# 创建训练集和测试集的数据加载器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4096, shuffle=True, drop_last = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4096, shuffle=False, drop_last = True)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 20\n",
    "lambda_u, lambda_b = 0.001, 0.001\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss_train, total_loss_test = 0.0, 0.0\n",
    "    for idx, (user_ids, movie_ids, ratings, tag_embedding) in tqdm(enumerate(train_dataloader)):\n",
    "        # 使用user_ids, movie_ids, ratings进行训练\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_ids.to(device), movie_ids.to(device))\n",
    "        loss = criterion(predictions, ratings.to(device)) + \\\n",
    "            lambda_u * (model.user_embedding.weight.norm(p = 2) + model.movie_embedding.weight.norm(p = 2)) + lambda_b * (model.user_bias.weight.norm(p = 2) + model.movie_bias.weight.norm(p = 2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "    output_loss_train = total_loss_train / (idx + 1) \n",
    "    results = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (user_ids, item_ids, true_ratings, tag_embedding) in enumerate(test_dataloader):\n",
    "            pred_ratings = model(user_ids.to(device), item_ids.to(device))\n",
    "            loss = criterion(pred_ratings, ratings.to(device))\n",
    "            total_loss_test += loss.item()\n",
    "            # 将结果转换为 numpy arrays\n",
    "            user_ids_np = user_ids.long().cpu().numpy().reshape(-1, 1)\n",
    "            pred_ratings_np = pred_ratings.cpu().numpy().reshape(-1, 1)\n",
    "            true_ratings_np = true_ratings.numpy().reshape(-1, 1)\n",
    "            # 将这三个 arrays 合并成一个 2D array\n",
    "            batch_results = np.column_stack((user_ids_np, pred_ratings_np, true_ratings_np))\n",
    "            # 将这个 2D array 添加到 results\n",
    "            results.append(batch_results)\n",
    "        # 将结果的 list 转换为一个大的 numpy array\n",
    "        results = np.vstack(results)\n",
    "        # 将结果转换为DataFrame\n",
    "        results_df = pd.DataFrame(results, columns=['user', 'pred', 'true'])\n",
    "        results_df['user'] = results_df['user'].astype(int)\n",
    "        # 按用户分组计算NDCG\n",
    "        ndcg_scores = results_df.groupby('user').apply(compute_ndcg)\n",
    "        # 计算平均NDCG\n",
    "        avg_ndcg = ndcg_scores.mean()\n",
    "        print(f'Epoch {epoch}, Train loss: {output_loss_train}, Test loss:, {total_loss_test / (idx + 1)}, Average NDCG: {avg_ndcg}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
